#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨é‡TikTokåˆ›ä½œè€…æœç´¢ç³»ç»Ÿ
æ”¯æŒä¸Šåƒæ¬¡æœç´¢ï¼Œå»é‡ä¼˜åŒ–ï¼ŒåŸºäºå®˜æ–¹é…ç½®
"""

import time
import json
import logging
import math
import os
from datetime import datetime
from typing import Dict, List, Set, Optional
from comprehensive_search_client import ComprehensiveSearchClient
from comprehensive_automation import TikTokCreatorAutomation
import pandas as pd

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FullScaleSearchEngine:
    """å…¨é‡æœç´¢å¼•æ“"""
    
    def __init__(self):
        self.client = ComprehensiveSearchClient()
        self.automation = TikTokCreatorAutomation()
        
        # å»é‡å’Œç¼“å­˜
        self.seen_creator_ids: Set[str] = set()
        self.seen_video_ids: Set[str] = set()
        self.collected_creators: List[Dict] = []
        
        # æœç´¢é…ç½® - åŸºäºå®˜æ–¹é…ç½®ä¼˜åŒ–
        self.qps_limit = 10  # æ¯ç§’10æ¬¡è¯·æ±‚
        self.request_interval = 0.1  # 100msé—´éš”ï¼Œæ”¯æŒ10 QPS
        self.max_retries = 3
        self.timeout = 45  # 45ç§’è¶…æ—¶
        
        # åˆ†é¡µæœç´¢é…ç½®
        self.max_per_search = 30  # æ¯æ¬¡æœç´¢30ä¸ªï¼Œä¿è¯ç¨³å®šæ€§
        self.max_offset = 10000  # æœ€å¤§åç§»é‡ï¼Œæ”¯æŒæ·±åº¦æœç´¢
        
        # æ–‡ä»¶ç¼–å·ç®¡ç†
        self.next_file_number = self._get_next_file_number()
    
    def _get_next_file_number(self) -> int:
        """è·å–ä¸‹ä¸€ä¸ªæ–‡ä»¶ç¼–å·"""
        if not os.path.exists("output"):
            return 1
        
        existing_numbers = []
        for filename in os.listdir("output"):
            # åŒ¹é…æ ¼å¼ï¼šæ•°å­—_å…³é”®è¯_æ—¶é—´æˆ³.csv æˆ– æ•°å­—_å…³é”®è¯_æ—¶é—´æˆ³_report.json
            if filename.endswith('.csv') or filename.endswith('_report.json'):
                parts = filename.split('_')
                if len(parts) >= 2:
                    try:
                        number = int(parts[0])
                        existing_numbers.append(number)
                    except ValueError:
                        continue
        
        if not existing_numbers:
            return 1
        
        return max(existing_numbers) + 1
        
    def full_scale_search(self, keyword: str, target_creators: int = 1000) -> List[Dict]:
        """
        å…¨é‡æœç´¢æŒ‡å®šå…³é”®è¯
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            target_creators: ç›®æ ‡åˆ›ä½œè€…æ•°é‡
            
        Returns:
            List[Dict]: å»é‡åçš„åˆ›ä½œè€…åˆ—è¡¨
        """
        logger.info(f"ğŸš€ å¼€å§‹å…¨é‡æœç´¢å…³é”®è¯: '{keyword}', ç›®æ ‡: {target_creators} ä¸ªåˆ›ä½œè€…")
        
        all_videos = []
        search_count = 0
        offset = 0
        consecutive_empty = 0
        
        while len(self.collected_creators) < target_creators and offset < self.max_offset:
            search_count += 1
            
            logger.info(f"ğŸ“¦ æœç´¢æ‰¹æ¬¡ {search_count}, offset={offset}, å·²æ”¶é›†={len(self.collected_creators)}")
            
            # æ‰§è¡Œåˆ†é¡µæœç´¢
            videos = self._paginated_search(keyword, offset=offset, count=self.max_per_search)
            
            if not videos:
                consecutive_empty += 1
                logger.warning(f"ç©ºç»“æœ {consecutive_empty}/3")
                
                if consecutive_empty >= 3:
                    logger.warning("è¿ç»­3æ¬¡ç©ºç»“æœï¼Œå¯èƒ½å·²è¾¾æœç´¢æé™")
                    break
                    
                # ç©ºç»“æœæ—¶ï¼Œè·³è·ƒæ›´å¤§çš„åç§»é‡
                offset += self.max_per_search * 2
                continue
            else:
                consecutive_empty = 0
                
            # å»é‡æ–°è§†é¢‘
            new_videos = self._deduplicate_videos(videos)
            all_videos.extend(new_videos)
            
            logger.info(f"æœ¬æ‰¹æ¬¡: {len(videos)} ä¸ªè§†é¢‘, å»é‡å: {len(new_videos)} ä¸ªæ–°è§†é¢‘")
            
            # æå–å’Œå»é‡åˆ›ä½œè€…
            new_creators = self._extract_and_deduplicate_creators(new_videos)
            self.collected_creators.extend(new_creators)
            
            # å¦‚æœè¶…è¿‡ç›®æ ‡æ•°é‡ï¼Œæˆªå–å‰Nä¸ªï¼ˆæŒ‰ç²‰ä¸æ•°æ’åºï¼‰
            if len(self.collected_creators) > target_creators:
                logger.info(f"ğŸ¯ è¾¾åˆ°ç›®æ ‡æ•°é‡ï¼Œæˆªå–å‰ {target_creators} ä¸ªåˆ›ä½œè€…")
                self.collected_creators = sorted(self.collected_creators, 
                                               key=lambda x: x.get('follower_count', 0), 
                                               reverse=True)[:target_creators]
                break
            
            logger.info(f"ç´¯è®¡è§†é¢‘: {len(all_videos)}, ç´¯è®¡åˆ›ä½œè€…: {len(self.collected_creators)}")
            
            # æ›´æ–°åç§»é‡
            offset += self.max_per_search
            
            # é€Ÿç‡é™åˆ¶
            time.sleep(self.request_interval)
            
            # æ¯100æ¬¡æœç´¢ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
            if search_count % 100 == 0:
                self._save_intermediate_results(keyword, search_count)
        
        logger.info(f"âœ… å…¨é‡æœç´¢å®Œæˆ: {search_count} æ¬¡æœç´¢, {len(all_videos)} ä¸ªè§†é¢‘, {len(self.collected_creators)} ä¸ªå”¯ä¸€åˆ›ä½œè€…")
        return self.collected_creators
    
    def _paginated_search(self, keyword: str, offset: int = 0, count: int = 30) -> List[Dict]:
        """æ‰§è¡Œåˆ†é¡µæœç´¢"""
        url = f"{self.client.base_url}/api/v1/tiktok/app/v3/fetch_general_search_result"
        
        params = {
            "keyword": keyword,
            "offset": offset,
            "count": count,
            "sort_type": 0,
            "publish_time": 0
        }
        
        # ä½¿ç”¨é‡è¯•æœºåˆ¶
        for attempt in range(self.max_retries):
            try:
                result = self.client._make_request(url, params)
                if result:
                    data = result.get("data", {})
                    items = data.get("data", [])
                    
                    videos = []
                    for item in items:
                        if item.get("type") == 1 and "aweme_info" in item:
                            videos.append(item["aweme_info"])
                    
                    return videos
                else:
                    logger.warning(f"æœç´¢å¤±è´¥ï¼Œå°è¯• {attempt + 1}/{self.max_retries}")
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    
            except Exception as e:
                logger.error(f"æœç´¢å¼‚å¸¸ {attempt + 1}/{self.max_retries}: {e}")
                time.sleep(2 ** attempt)
        
        return []
    
    def _deduplicate_videos(self, videos: List[Dict]) -> List[Dict]:
        """å»é‡è§†é¢‘"""
        new_videos = []
        for video in videos:
            video_id = video.get("aweme_id", "")
            if video_id and video_id not in self.seen_video_ids:
                self.seen_video_ids.add(video_id)
                new_videos.append(video)
        return new_videos
    
    def _extract_and_deduplicate_creators(self, videos: List[Dict]) -> List[Dict]:
        """æå–å¹¶å»é‡åˆ›ä½œè€…"""
        new_creators = []
        
        for video in videos:
            author = video.get("author", {})
            if not author:
                continue
            
            # å¤šé‡å»é‡é”®
            unique_id = author.get("unique_id", "")
            user_id = author.get("uid", "")
            sec_uid = author.get("sec_uid", "")
            
            # ç”Ÿæˆå»é‡é”®
            creator_key = unique_id or user_id or sec_uid
            if not creator_key or creator_key in self.seen_creator_ids:
                continue
            
            # ç²‰ä¸æ•°è¿‡æ»¤
            follower_count = author.get("follower_count", 0)
            if follower_count < 1000:
                continue
            
            self.seen_creator_ids.add(creator_key)
            
            # æå–åˆ›ä½œè€…åŸºæœ¬ä¿¡æ¯
            creator_info = {
                "user_id": user_id,
                "sec_user_id": sec_uid,
                "nickname": author.get("nickname", ""),
                "unique_id": unique_id,
                "signature": author.get("signature", ""),
                "follower_count": follower_count,
                "following_count": author.get("following_count", 0),
                "aweme_count": author.get("aweme_count", 0),
                "total_favorited": author.get("total_favorited", 0),
                "avatar_url": author.get("avatar_larger", {}).get("url_list", [""])[0] if author.get("avatar_larger") else "",
                "verified": author.get("verification_type", 0) > 0,
                "tiktok_account_url": self.client._generate_tiktok_url(unique_id)
            }
            
            new_creators.append(creator_info)
        
        return new_creators
    
    def _save_intermediate_results(self, keyword: str, search_count: int):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"output/intermediate_{keyword}_{search_count}_{timestamp}.json"
        
        data = {
            "keyword": keyword,
            "search_count": search_count,
            "total_creators": len(self.collected_creators),
            "total_videos": len(self.seen_video_ids),
            "creators": self.collected_creators
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: {filename}")
        except Exception as e:
            logger.error(f"ä¿å­˜ä¸­é—´ç»“æœå¤±è´¥: {e}")
    
    def enhance_creators_batch(self, creators: List[Dict], batch_size: int = 50) -> List[Dict]:
        """æ‰¹é‡å¢å¼ºåˆ›ä½œè€…æ•°æ®"""
        logger.info(f"ğŸ”§ å¼€å§‹æ‰¹é‡å¢å¼º {len(creators)} ä¸ªåˆ›ä½œè€…çš„æ•°æ®")
        
        enhanced_creators = []
        
        for i in range(0, len(creators), batch_size):
            batch = creators[i:i+batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(creators) + batch_size - 1) // batch_size
            
            logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} ä¸ªåˆ›ä½œè€…)")
            
            for j, creator in enumerate(batch, 1):
                try:
                    logger.info(f"ğŸ“Š å¢å¼ºåˆ›ä½œè€… {i+j}/{len(creators)}: {creator.get('nickname', 'Unknown')}")
                    enhanced = self.client.enhance_creator_data(creator)
                    
                    if enhanced:
                        # åº”ç”¨å­—æ®µé¡ºåºå’Œæ•°æ®å¤„ç†
                        processed = self._process_creator_data(enhanced)
                        enhanced_creators.append(processed)
                    
                except Exception as e:
                    logger.error(f"å¢å¼ºåˆ›ä½œè€…æ•°æ®å¤±è´¥: {e}")
                    continue
            
            # æ‰¹æ¬¡é—´ä¼‘æ¯
            if batch_num < total_batches:
                logger.info(f"â³ æ‰¹æ¬¡å®Œæˆï¼Œä¼‘æ¯1ç§’...")
                time.sleep(1)
        
        logger.info(f"âœ… æ‰¹é‡å¢å¼ºå®Œæˆ: {len(enhanced_creators)}/{len(creators)} æˆåŠŸ")
        return enhanced_creators
    
    def _process_creator_data(self, enhanced: Dict) -> Dict:
        """å¤„ç†åˆ›ä½œè€…æ•°æ®ï¼Œåº”ç”¨å­—æ®µé¡ºåº"""
        # è®¡ç®—å¹³å‡æ’­æ”¾æ•°
        video_1_count = enhanced.get("video_1_play_count", 0)
        video_2_count = enhanced.get("video_2_play_count", 0)
        video_3_count = enhanced.get("video_3_play_count", 0)
        video_4_count = enhanced.get("video_4_play_count", 0)
        video_5_count = enhanced.get("video_5_play_count", 0)
        
        video_counts = [video_1_count, video_2_count, video_3_count, video_4_count, video_5_count]
        valid_counts = [count for count in video_counts if count > 0]
        avg_play_count = sum(valid_counts) / len(valid_counts) if valid_counts else 0
        
        # è®¡ç®—ä¸­ä½æ•°æ’­æ”¾é‡
        if valid_counts:
            sorted_counts = sorted(valid_counts)
            n = len(sorted_counts)
            if n % 2 == 0:
                # å¶æ•°ä¸ªæ•°ï¼Œå–ä¸­é—´ä¸¤ä¸ªæ•°çš„å¹³å‡å€¼
                median_view_count = (sorted_counts[n//2 - 1] + sorted_counts[n//2]) / 2
            else:
                # å¥‡æ•°ä¸ªæ•°ï¼Œå–ä¸­é—´çš„æ•°
                median_view_count = sorted_counts[n//2]
        else:
            median_view_count = 0
        
        # è®¡ç®—é¢„æœŸä»·æ ¼: ä½¿ç”¨æ–°çš„å¤æ‚å…¬å¼
        # weighted_views = 0.4 * V1 + 0.25 * V2 + 0.15 * V3 + 0.1 * V4 + 0.1 * V5
        weights = [0.4, 0.25, 0.15, 0.1, 0.1]
        weighted_views = 0
        for i, count in enumerate([video_1_count, video_2_count, video_3_count, video_4_count, video_5_count]):
            if i < len(weights) and count > 0:
                weighted_views += weights[i] * count
        
        # follower_factor = log10(follower_count + 1)
        follower_count = enhanced.get("follower_count", 0)
        follower_factor = math.log10(follower_count + 1) if follower_count >= 0 else 0
        
        # engagement_rate = (total_likes_count / total_video_count) / avg_video_play_count
        total_likes = enhanced.get("total_favorited", 0)
        total_videos = enhanced.get("aweme_count", 1)  # é¿å…é™¤é›¶
        if total_videos > 0 and avg_play_count > 0:
            engagement_rate = (total_likes / total_videos) / avg_play_count
        else:
            engagement_rate = 0
        
        # final_price = max(80, weighted_views/1000 * 1 + follower_factor * 5 + engagement_rate * 50)
        calculated_price = (weighted_views / 1000) * 1 + follower_factor * 5 + engagement_rate * 50
        expected_price = max(80, calculated_price)
        
        # æå–é‚®ç®±
        bio_description = enhanced.get("signature", "")
        email = self.client._extract_email_from_bio(bio_description)
        if not email:
            email = ""
        
        # è·å–æ´»è·ƒåº¦
        days_since_last_video = enhanced.get("days_since_last_video", -1)
        
        # æŒ‰è¦æ±‚çš„å­—æ®µé¡ºåº
        return {
            "search_keyword": "temp",  # åç»­ä¼šæ›´æ–°
            "nickname": enhanced.get("nickname", ""),
            "unique_id": enhanced.get("unique_id", ""),
            "follower_count": enhanced.get("follower_count", 0),
            "total_video_count": enhanced.get("aweme_count", 0),
            "total_likes_count": enhanced.get("total_favorited", 0),
            "avg_video_play_count": int(avg_play_count),
            "median_view_count": int(median_view_count),
            "expected_price": round(expected_price, 2),
            "days_since_last_video": days_since_last_video,
            "tiktok_account_url": enhanced.get("tiktok_account_url", ""),
            "tiktok_account_bio_description": bio_description,
            "email": email,
            "bio_link_url": enhanced.get("bio_link_url", ""),
            "language": enhanced.get("language", ""),
            "latest_video_link": enhanced.get("video_1_link", ""),
            "latest_video_play_count": video_1_count,
            "second_latest_video_link": enhanced.get("video_2_link", ""),
            "second_latest_video_play_count": video_2_count,
            "third_latest_video_link": enhanced.get("video_3_link", ""),
            "third_latest_video_play_count": video_3_count,
            "fourth_latest_video_link": enhanced.get("video_4_link", ""),
            "fourth_latest_video_play_count": video_4_count,
            "fifth_latest_video_link": enhanced.get("video_5_link", ""),
            "fifth_latest_video_play_count": video_5_count
        }
    
    def save_results(self, keyword: str, enhanced_creators: List[Dict], output_format: str = "csv", keyword_index: int = 1) -> Dict[str, str]:
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # ä½¿ç”¨åŠ¨æ€æ–‡ä»¶ç¼–å·ï¼ŒåŸºäºç°æœ‰æ–‡ä»¶è‡ªåŠ¨é€’å¢
        file_number = self.next_file_number
        self.next_file_number += 1  # ä¸ºä¸‹ä¸€ä¸ªæ–‡ä»¶å‡†å¤‡ç¼–å·
        file_prefix = f"output/{file_number:02d}_{keyword}_{timestamp}"
        
        # æ›´æ–°search_keywordå­—æ®µ
        for creator in enhanced_creators:
            creator["search_keyword"] = keyword
        
        file_paths = {}
        
        # ä¿å­˜CSV
        if output_format in ["csv", "both"]:
            csv_file = f"{file_prefix}.csv"
            df = pd.DataFrame(enhanced_creators)
            df = df.fillna('')  # å¤„ç†NaNå€¼
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            file_paths["csv_file"] = csv_file
            logger.info(f"âœ… CSVæ–‡ä»¶å·²ä¿å­˜: {csv_file}")
        
        # ä¿å­˜Excel
        if output_format in ["excel", "both"]:
            excel_file = f"{file_prefix}.xlsx"
            df = pd.DataFrame(enhanced_creators)
            df = df.fillna('')
            df.to_excel(excel_file, index=False, sheet_name='åˆ›ä½œè€…å®Œæ•´ä¿¡æ¯')
            file_paths["excel_file"] = excel_file
            logger.info(f"âœ… Excelæ–‡ä»¶å·²ä¿å­˜: {excel_file}")
        
        # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
        report_file = f"{file_prefix}_report.json"
        report = self._generate_report(keyword, enhanced_creators)
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        file_paths["report_file"] = report_file
        
        return file_paths
    
    def _generate_report(self, keyword: str, creators: List[Dict]) -> Dict:
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        if not creators:
            return {"error": "æ— æ•°æ®"}
        
        df = pd.DataFrame(creators)
        
        return {
            "keyword": keyword,
            "collection_time": datetime.now().isoformat(),
            "total_searches": len(self.seen_video_ids),
            "total_creators": len(creators),
            "total_followers": int(df["follower_count"].sum()),
            "avg_followers": float(df["follower_count"].mean()),
            "total_videos": int(df["total_video_count"].sum()),
            "total_likes": int(df["total_likes_count"].sum()),
            "avg_play_count": float(df["avg_video_play_count"].mean()),
            "email_count": len(df[df["email"] != ""]),
            "active_creators": len(df[df["days_since_last_video"] <= 30]),
            "top_creators": [
                {
                    "nickname": str(row["nickname"]),
                    "unique_id": str(row["unique_id"]),
                    "follower_count": int(row["follower_count"])
                }
                for _, row in df.nlargest(10, "follower_count").iterrows()
            ]
        }

def process_single_keyword(engine, keyword: str, target_creators: int, enhance_data: bool, output_format: str) -> dict:
    """å¤„ç†å•ä¸ªå…³é”®è¯çš„æœç´¢"""
    print(f"\nğŸ” å¼€å§‹æœç´¢å…³é”®è¯: '{keyword}'")
    print(f"ç›®æ ‡æ•°é‡: {target_creators} | å¢å¼ºæ•°æ®: {'æ˜¯' if enhance_data else 'å¦'}")
    print("-" * 50)
    
    start_time = time.time()
    
    # é‡ç½®å¼•æ“çŠ¶æ€ï¼ˆæ¸…ç©ºä¹‹å‰çš„æ”¶é›†ç»“æœï¼‰
    engine.seen_creator_ids.clear()
    engine.seen_video_ids.clear()
    engine.collected_creators.clear()
    
    # ç¬¬ä¸€é˜¶æ®µï¼šå…¨é‡æœç´¢
    print("ğŸ“¦ ç¬¬ä¸€é˜¶æ®µ: å…¨é‡æœç´¢åˆ›ä½œè€…...")
    raw_creators = engine.full_scale_search(keyword, target_creators)
    
    if not raw_creators:
        print(f"âŒ å…³é”®è¯ '{keyword}' æœªæ‰¾åˆ°ä»»ä½•åˆ›ä½œè€…")
        return {"keyword": keyword, "success": False, "creators": [], "file_paths": {}, "time": 0}
    
    phase1_time = time.time() - start_time
    print(f"âœ… ç¬¬ä¸€é˜¶æ®µå®Œæˆ: {len(raw_creators)} ä¸ªåˆ›ä½œè€…, ç”¨æ—¶ {phase1_time/60:.1f} åˆ†é’Ÿ")
    
    # ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®å¢å¼º
    if enhance_data:
        print(f"ğŸ“Š ç¬¬äºŒé˜¶æ®µ: æ•°æ®å¢å¼º...")
        enhanced_creators = engine.enhance_creators_batch(raw_creators)
    else:
        # ä¸å¢å¼ºæ•°æ®ï¼Œç›´æ¥å¤„ç†
        enhanced_creators = []
        for creator in raw_creators:
            processed = engine._process_creator_data(creator)
            enhanced_creators.append(processed)
    
    total_time = time.time() - start_time
    
    # ç¡®ä¿ç²¾ç¡®æ•°é‡æ§åˆ¶
    if len(enhanced_creators) > target_creators:
        print(f"âš¡ æ•°é‡æ§åˆ¶: ä» {len(enhanced_creators)} ä¸ªå‡å°‘åˆ° {target_creators} ä¸ª")
        print(f"ğŸ“Š æ’åºæ–¹å¼: æŒ‰ç²‰ä¸æ•°é™åºæ’åˆ—ï¼Œå–å‰ {target_creators} å")
        enhanced_creators = sorted(enhanced_creators, 
                                 key=lambda x: x.get('follower_count', 0), 
                                 reverse=True)[:target_creators]
    
    # ä¿å­˜ç»“æœ
    file_paths = engine.save_results(keyword, enhanced_creators, output_format)
    
    # æ˜¾ç¤ºç»“æœ
    print(f"âœ… å…³é”®è¯ '{keyword}' æœç´¢å®Œæˆ!")
    print(f"â±ï¸ ç”¨æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
    print(f"ğŸ“Š ç»“æœ: {len(enhanced_creators)} ä¸ªåˆ›ä½œè€…")
    
    if enhanced_creators:
        df = pd.DataFrame(enhanced_creators)
        print(f"ğŸ“ˆ ç»Ÿè®¡: å¹³å‡ç²‰ä¸ {df['follower_count'].mean():,.0f} | é‚®ç®± {len(df[df['email'] != ''])} ä¸ª")
    
    return {
        "keyword": keyword,
        "success": True,
        "creators": enhanced_creators,
        "file_paths": file_paths,
        "time": total_time
    }

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ TikTokå…¨é‡åˆ›ä½œè€…æœç´¢ç³»ç»Ÿ")
    print("æ”¯æŒå¤šå…³é”®è¯æœç´¢ï¼Œæ™ºèƒ½å»é‡ï¼ŒåŸºäºå®˜æ–¹é…ç½®ä¼˜åŒ–")
    print("="*70)
    
    engine = FullScaleSearchEngine()
    
    try:
        # è·å–æœç´¢å‚æ•°
        keywords_input = input("è¯·è¾“å…¥æœç´¢å…³é”®è¯(å¤šä¸ªç”¨é€—å·åˆ†éš”): ").strip()
        if not keywords_input:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆå…³é”®è¯")
            return
        
        # è§£æå¤šä¸ªå…³é”®è¯ï¼Œå¤„ç†ç©ºæ ¼
        keywords = [kw.strip() for kw in keywords_input.split(',') if kw.strip()]
        if not keywords:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆå…³é”®è¯")
            return
        
        print(f"ğŸ“ è§£æåˆ° {len(keywords)} ä¸ªå…³é”®è¯: {keywords}")
        
        try:
            target_creators = int(input("æ¯ä¸ªå…³é”®è¯çš„ç›®æ ‡åˆ›ä½œè€…æ•°é‡ï¼ˆé»˜è®¤20ï¼‰: ") or "20")
        except ValueError:
            target_creators = 20
        
        output_format = input("è¾“å‡ºæ ¼å¼ (csv/excel/bothï¼Œé»˜è®¤csv): ").strip().lower() or "csv"
        
        # æ˜¯å¦å¢å¼ºæ•°æ®
        enhance_choice = input("æ˜¯å¦è·å–è¯¦ç»†è§†é¢‘æ•°æ®? (ä¼šæ˜¾è‘—å¢åŠ æ—¶é—´) (y/nï¼Œé»˜è®¤y): ").strip().lower() or "y"
        enhance_data = enhance_choice in ['y', 'yes', 'æ˜¯']
        
        print(f"\nğŸ¯ æœç´¢é…ç½®:")
        print(f"å…³é”®è¯æ•°é‡: {len(keywords)}")
        print(f"æ¯ä¸ªå…³é”®è¯ç›®æ ‡: {target_creators} ä¸ªåˆ›ä½œè€…")
        print(f"å¢å¼ºæ•°æ®: {'æ˜¯' if enhance_data else 'å¦'}")
        print(f"è¾“å‡ºæ ¼å¼: {output_format}")
        print(f"é…ç½®: QPS=10, é‡è¯•=3æ¬¡, è¶…æ—¶=45s")
        print("="*70)
        
        overall_start_time = time.time()
        all_results = []
        total_creators = 0
        
        # é€ä¸ªå¤„ç†å…³é”®è¯
        for i, keyword in enumerate(keywords, 1):
            print(f"\nğŸš€ å¤„ç†å…³é”®è¯ {i}/{len(keywords)}: '{keyword}'")
            
            result = process_single_keyword(engine, keyword, target_creators, enhance_data, output_format)
            all_results.append(result)
            
            if result["success"]:
                total_creators += len(result["creators"])
                print(f"ğŸ“ ç”Ÿæˆæ–‡ä»¶:")
                for file_type, path in result["file_paths"].items():
                    print(f"  ğŸ“„ {file_type}: {path}")
            
            # å…³é”®è¯é—´ä¼‘æ¯ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
            if i < len(keywords):
                print(f"â³ å‡†å¤‡å¤„ç†ä¸‹ä¸€ä¸ªå…³é”®è¯...")
                time.sleep(2)
        
        overall_time = time.time() - overall_start_time
        
        # æ€»ç»“æŠ¥å‘Š
        print(f"\nğŸ‰ å¤šå…³é”®è¯æœç´¢å®Œæˆ!")
        print(f"â±ï¸ æ€»ç”¨æ—¶: {overall_time/60:.1f} åˆ†é’Ÿ")
        print(f"ğŸ“Š æ€»ä½“ç»“æœ:")
        print(f"  â€¢ å¤„ç†å…³é”®è¯: {len(keywords)} ä¸ª")
        print(f"  â€¢ æˆåŠŸå…³é”®è¯: {sum(1 for r in all_results if r['success'])} ä¸ª")
        print(f"  â€¢ æ€»åˆ›ä½œè€…æ•°: {total_creators} ä¸ª")
        print(f"  â€¢ å¹³å‡æ¯ä¸ªå…³é”®è¯: {total_creators/len(keywords):.1f} ä¸ª")
        
        # æ˜¾ç¤ºæ¯ä¸ªå…³é”®è¯çš„ç»“æœ
        print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
        for result in all_results:
            status = "âœ…" if result["success"] else "âŒ"
            creators_count = len(result["creators"]) if result["success"] else 0
            print(f"  {status} '{result['keyword']}': {creators_count} ä¸ªåˆ›ä½œè€…")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        if hasattr(engine, 'collected_creators') and engine.collected_creators:
            print(f"ğŸ’¾ ä¿å­˜å·²æ”¶é›†çš„ {len(engine.collected_creators)} ä¸ªåˆ›ä½œè€…...")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå‡ºé”™: {e}")
        logger.error(f"ç¨‹åºå‡ºé”™: {e}", exc_info=True)

if __name__ == "__main__":
    main()